{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Information Extraction - Assignment 2\n",
    "\n",
    "<b>Student Id: 17230755</b>\n",
    "\n",
    "<b>Name: Swaroop S Bhat</b>\n",
    "\n",
    "### Overview:\n",
    "This assignment is regarding Information extraction in a football player data set. I have used a standford tagger for Named Entity extraction. \n",
    "\n",
    "Please Note:\n",
    "\n",
    "    - Need to set environment to os.environ['JAVAHOME'] = \"C:/Program Files/Java/jre1.8.0_144/bin/\" (corresponding relative path should be given according to the system)\n",
    "    - Need to downlaod the standfor NER tagger from the site here - http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\n",
    "    - If this dosent work - please uncomment the code in name_0f_the_player function. Instruction is given in comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all necessary tools required\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import itertools\n",
    "from pyld import jsonld\n",
    "import json\n",
    "import unidecode\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger('C:/Users/Swaroop Bhat/Anaconda3/stanford-ner-2014-06-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'C:/Users/Swaroop Bhat/Anaconda3/stanford-ner-2014-06-16/stanford-ner.jar')\n",
    "os.environ['JAVAHOME'] = \"C:/Program Files/Java/jre1.8.0_144/bin/\"\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the file:\n",
    "\n",
    "Please make sure txt file is in working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please make sure the txt file is in home directory\n",
    "inputfile='football_players.txt'\n",
    "buf=open(inputfile, encoding=\"UTF-8\")\n",
    "list_of_doc=buf.read().split('\\n')\n",
    "\n",
    "\n",
    "# Removing all empty lines i.e empty lines between paragraph\n",
    "l = []\n",
    "for i in list_of_doc:\n",
    "    if len(i) != 0:\n",
    "        l.append(i)\n",
    "list_of_doc = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (10 Marks)\n",
    "Write a function that takes each document and performs:\n",
    "1) sentence segmentation 2) tokenization 3) part-of-speech tagging\n",
    "\n",
    "Please keep in mind that the expected output is a list within a list as shown below.\n",
    "\n",
    "All abobve mentioned process is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    \n",
    "    '''This function pos tag the sentences and stores in the list'''\n",
    "    \n",
    "    # Convert string into list of sentences\n",
    "    sentence = sent_tokenize(document)\n",
    "    pos_sentences = []\n",
    "    try:\n",
    "        for i in sentence:\n",
    "            # Tokenize each sentence\n",
    "            text = word_tokenize(i)\n",
    "            # Tag the tokenizeed sentence\n",
    "            pos_sentences.append(nltk.pos_tag(text))\n",
    "        return pos_sentences\n",
    "    except:\n",
    "        print(\"Please make sure that input to the function is of type string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('forward', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('serves', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('captain', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Portugal', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc=list_of_doc[0]\n",
    "# Calling the ie_preprocess function\n",
    "pos_sent = ie_preprocess(first_doc)\n",
    "\n",
    "# Display all tagged sentences as given in the assignment sheet. However, to dissplay all tagged sentence, please remove the \n",
    "# list index\n",
    "pos_sent[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (20 Marks)\n",
    "Write a function that will take the list of tokens with POS tags for each sentence and returns the named entities (NE). \n",
    "\n",
    "Hint: Use binary=True while calling NE chunk function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cristiano Ronaldo',\n",
       " 'Santos Aveiro',\n",
       " 'ComM',\n",
       " 'GOIH',\n",
       " 'Portuguese',\n",
       " 'Spanish',\n",
       " 'Real Madrid',\n",
       " 'Portugal']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def named_entity_finding(pos_sent, x=True, y=\"NE\"):\n",
    "    \n",
    "    \"\"\"This functions identifies the named entities in a given pos tagged sentence. This logic is used in another \n",
    "    function to change binary=True in ne_chunk. Hence given the default parameters in the function. \n",
    "    Also using this we can find the NE labeled - person or location... etc default is person.\"\"\" \n",
    "    \n",
    "    # This is check if the return needs all NE or specific NE - default type is PERSON.\n",
    "    if y != \"NE\":\n",
    "        x=False\n",
    "        if y==\"NE\":\n",
    "            y = \"PERSON\"\n",
    "    myNE = []\n",
    "    \n",
    "    \n",
    "    # The below logic finds all named entities like, PERSON, ORGANIZATION, LOCATION etc\n",
    "    try:\n",
    "        tree = nltk.ne_chunk(pos_sent, binary=x)\n",
    "\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == y:\n",
    "                entity = \"\"\n",
    "                for leaf in subtree.leaves():\n",
    "                    entity = entity + leaf[0] + \" \"\n",
    "                myNE.append(entity.strip())\n",
    "\n",
    "        return myNE\n",
    "    except:\n",
    "        print(\"Please make sure to give parsed tree as input to the function.\")\n",
    "\n",
    "        \n",
    "# Calling the above function - specific document need to be passed.\n",
    "pos_sents=ie_preprocess(list_of_doc[0])\n",
    "# passing the first sentence of document 4 to findd nam\n",
    "named_entity_finding(pos_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (10 Marks)\n",
    "\n",
    "Now use the named_entity_finding() function to extract all NEs for each document.\n",
    "\n",
    "Hint: pos_sents holds the list of lists of tokens with POS tags.\n",
    "\n",
    "Note: Only few list of NE is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UEFA European',\n",
       " 'British',\n",
       " 'FIFA Pusk√°s Award',\n",
       " 'Spain',\n",
       " 'German',\n",
       " 'Dagens Nyheter',\n",
       " 'Ronaldo',\n",
       " 'Marco',\n",
       " 'OBE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NE_flat_list_fn(document):\n",
    "    \"\"\"This function is extracts named entities in entire document and returns all unique NE as flattened list\"\"\"\n",
    "    NE=[]\n",
    "    try:\n",
    "        # Loop through each docudment 1-10\n",
    "        for i in document:\n",
    "            # POS tag the document\n",
    "            pos_sents = ie_preprocess(i)\n",
    "            for pos_sent in pos_sents:\n",
    "                # For each tagged sentence in document find named entity\n",
    "                entity = named_entity_finding(pos_sent)\n",
    "                if len(entity) != 0:\n",
    "                    NE.append(entity)\n",
    "\n",
    "        \n",
    "        # This function flattens the list\n",
    "        NE_flat_list = list(itertools.chain.from_iterable(NE))\n",
    "        return NE_flat_list\n",
    "    except:     \n",
    "        print(\"Please make sure that input to the function is off type string\")\n",
    "\n",
    "        \n",
    "ne_flat = NE_flat_list_fn(list_of_doc)\n",
    "\n",
    "# Note set is used to return the unique named entities - only 10 named entites are displayed here. \n",
    "# There are multiple same named entities - However, for any process - if we take the unique NE it is better in tems of memory. \n",
    "# This set can also be declared instind the NE_flat_list_n.\n",
    "list(set(ne_flat))[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (40 Marks)\n",
    "\n",
    "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
    "\n",
    "Hint: Use the re.compile() function to create the extraction patterns\n",
    "\n",
    "Reference: https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo dos Santos\n"
     ]
    }
   ],
   "source": [
    "def name_of_the_player(doc):\n",
    "    \n",
    "    \"\"\"This function finds the person name\"\"\"\n",
    "    \n",
    "    t = []\n",
    "    name = []\n",
    "    try:\n",
    "        pos_sents = ie_preprocess(doc)\n",
    "        tree = nltk.ne_chunk(pos_sents[0])\n",
    "        \n",
    "        \n",
    "        \"\"\" This is to extract player name using standfor NE tagger. Standford works slightly better than NLTK default tagger.\n",
    "        Moreover, Standfor NE tagger detects the surname as well.\n",
    "        E.g: NLTK default - Cristano Ronaldo : retuns Crisstano as PERSON and Ronaldo as ORGANIZATION. Hence using standford package.\"\"\"\n",
    "        \n",
    "        sentence = unidecode.unidecode(sent_tokenize(doc)[0])\n",
    "        tag = st.tag(sentence.split())\n",
    "        for i in tag:\n",
    "            if i[1] == \"PERSON\":\n",
    "                t.append(i[0])\n",
    "        \n",
    "        return \" \".join(t)\n",
    "\n",
    "        \n",
    "        \"\"\"Note: If not able to install standfor NE tagger - please uncomment the below section (while commenting above block)\"\"\"\n",
    "        \n",
    "        #for subtree in tree.subtrees():\n",
    "            #if subtree.label() == \"PERSON\":\n",
    "                #entity = \"\"\n",
    "                #for leaf in subtree.leaves():\n",
    "                    #entity = entity + leaf[0] + \" \"\n",
    "                #name.append(entity.strip())\n",
    "\n",
    "        #return \" \".join(name)\n",
    "    \n",
    "    except:\n",
    "        print(\"Please make sure the entered documnent is of type string.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(name_of_the_player(list_of_doc[0]))\n",
    "except:\n",
    "    print(\"Please make sure give list index is right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portugal\n"
     ]
    }
   ],
   "source": [
    "def country_of_origin(doc):\n",
    "    \n",
    "    \"\"\" Three levels of filter is applied here. Because NLTK NE tagger dosent recognise all the country names. Moreover, \n",
    "    we can use look up to get the country name (because the text contains like spanish that can be converted to Spain).\n",
    "    LEVEL 1 - Used NLTK tagger to find Named Entity. If not found go to LEVEL - 2\n",
    "    LEVEL 2 - Used Regex to check for word with profesional footballer. If not found go to LEVEL - 3\n",
    "    LEVEL 3 - USed Regex to match word with National Team.\n",
    "    Note - Not used Look up here. This type of Levels is useful in a large documents\"\"\"\n",
    "    \n",
    "    sentence = sent_tokenize(doc)[0]\n",
    "    tm = []\n",
    "    seq = []\n",
    "    \n",
    "    try:   \n",
    "        # LEVEL 2 (look for prrofesssional footballer.)\n",
    "        # This uses regex to find the country of origin if NLTK fails to detect GPE or LOCATION tag.\n",
    "        m = re.search(r'((?:\\w+\\W+){,3})(footballer)', sentence)\n",
    "        s = \"\".join(list(m.groups()))\n",
    "        mal = re.search(r'((\\w+ ){1})professional', s)\n",
    "        # Take the first group most probably the country name\n",
    "        try:\n",
    "            mal = mal.groups()[0]\n",
    "        except:\n",
    "            mal = None\n",
    "\n",
    "        # LEVEL - 1 (Primary lever, get NE for 1st sentence and look for GPE)\n",
    "        # Get tag for first sentence\n",
    "        tag = ie_preprocess(s)[0]\n",
    "        tree = nltk.ne_chunk(tag)\n",
    "        born = []\n",
    "\n",
    "        # Check for GPE tag if any (sometimes NLTK dosent detect GPE).\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'GPE':\n",
    "                entity = \"\"\n",
    "                for leaf in subtree.leaves():\n",
    "                    entity = entity + leaf[0] + \" \"\n",
    "                born.append(entity.strip())\n",
    "\n",
    "\n",
    "        # LEVEL 3 (if nowhere found then check for words with national team.)\n",
    "        match = re.compile(r'((?:[\\S,]+\\s+){0,1})national team')\n",
    "        for i, sent in enumerate(sent_tokenize(doc)):\n",
    "            if i == 2:\n",
    "                break\n",
    "            team = match.findall(sent)\n",
    "            if len(team) != 0:\n",
    "                tm.append(team[0])\n",
    "\n",
    "\n",
    "        # If NLTK fails to detect then use Regex re.search(r'((?:\\w+\\W+){,3})(footballer)', sentence) - i.e take 3 words before \n",
    "        # the word footballer and take the first group.\n",
    "        if len(born) == 0:\n",
    "            if len(mal) != 0:\n",
    "                born = nltk.word_tokenize(mal) \n",
    "            elif len(tm) != 0:\n",
    "                born = tm\n",
    "        \n",
    "        # Lookup of all the countries - can use all country name as look up but included only few.\n",
    "        # sequence matcher is used to find the country in look up.\n",
    "        l = ['Spain', 'England', 'Portugal', 'Wales', 'England', 'Brazil', 'Germany', 'Argentina', 'Sweedan']\n",
    "        for x in l:\n",
    "            seq.append(SequenceMatcher(None, born[0], x).ratio())\n",
    "        final = l[seq.index(max(seq))]\n",
    "            \n",
    "        \n",
    "        return final\n",
    "    except:\n",
    "        print(\"Please make sure the data given to this function is of type string\")\n",
    "\n",
    "try:\n",
    "    print(country_of_origin(list_of_doc[0]))\n",
    "except:\n",
    "    print(\"Please make sure the given index is right!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 February 1992 \n"
     ]
    }
   ],
   "source": [
    "def date_of_birth(doc):\n",
    "    \n",
    "    \"\"\"Used regex to find the date of bith. First extract the sentence containing born and find the matching pattern.\"\"\"\n",
    "    \n",
    "    sentence = sent_tokenize(doc)[0]\n",
    "    match = re.compile(r'born\\b\\s*((?:\\S+\\s+){0,3})')\n",
    "    born = match.findall(sentence)[0]\n",
    "    born = re.sub('\\W+',' ', born )\n",
    "    \n",
    "    return born\n",
    "\n",
    "\n",
    "try:\n",
    "    print(date_of_birth(list_of_doc[2]))\n",
    "except:\n",
    "    print(\"Please make sure the list indexing is right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spain national team', 'FC Barcelona']\n"
     ]
    }
   ],
   "source": [
    "def team_of_the_player(doc):\n",
    "    \n",
    "    \"\"\"This functions returns teams of the player. Note Some of the players dosent have national team information\n",
    "    (for sentence 4). Hence, assumed as player plays for his country of origin (This may not be true for always). \"\"\"\n",
    "    \n",
    "    sentence = sent_tokenize(doc)\n",
    "    tm = []\n",
    "    club = []\n",
    "    club2 = []\n",
    "    \n",
    "    try:\n",
    "        tag = ie_preprocess(doc)\n",
    "        # Check for only first 2 sentences. Saves time and efficent.\n",
    "        for i, sent in enumerate(sentence):\n",
    "            if i == 2:\n",
    "                break\n",
    "            # Extract the national team of the player using below regex.\n",
    "            match = re.compile(r'((?:[\\S,]+\\s+){0,1})national team')\n",
    "            \n",
    "            # This is to extract the clubs of the player.\n",
    "            match_club = re.compile(r'club\\s+((?:[\\S,]+\\s*){0,2})')\n",
    "            \n",
    "            # Find the national team\n",
    "            team = match.findall(sent)\n",
    "            \n",
    "            # If club keyword found in first couple of sentences. Extract those clubs.\n",
    "            if len(match_club.findall(sent)) != 0:\n",
    "                for f in match_club.findall(sent):\n",
    "                    club2.append(f)\n",
    "            \n",
    "            # Search of clubs. This can be found by checking for organization by calling Named Entity function and type is \n",
    "            # ORGANIZATION.\n",
    "            club.append(named_entity_finding(tag[i], False, \"ORGANIZATION\"))\n",
    "            if len(team) != 0:\n",
    "                l = team[0]+\"national team\"\n",
    "                tm.append(l)\n",
    "\n",
    "        # Find unique names because sentence may have same names multiple times.\n",
    "        if len(list(set(tm))) != 0:\n",
    "            national_team = list(set(tm))[0]\n",
    "        else:\n",
    "            national_team = country_of_origin(doc) +\" national team\"\n",
    "\n",
    "        # Flattens the list\n",
    "        club = list(itertools.chain.from_iterable(club))\n",
    "        \n",
    "        # Finding the most probable club names by doing intersection of two process.\n",
    "        nclub = []\n",
    "        for q in club2:\n",
    "            nclub.extend(nltk.word_tokenize(q))\n",
    "\n",
    "        for i, s in enumerate(club2):\n",
    "            club2[i] = club2[i].rstrip()\n",
    "        if len(list(set(club).intersection(club2))) != 0:\n",
    "            club = list(set(club).intersection(club2))\n",
    "        club.append(national_team)\n",
    "\n",
    "\n",
    "        club = list(set(club).difference(nltk.word_tokenize(sentence[0])[0:6]))\n",
    "\n",
    "        return club\n",
    "    except:\n",
    "        print(\"Please make sure the data given to this function is of type string\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(team_of_the_player(list_of_doc[9]))\n",
    "except:\n",
    "    print(\"Please make sure the given list index is right.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forward']\n"
     ]
    }
   ],
   "source": [
    "def position_of_the_player(doc):\n",
    "    \n",
    "    \"\"\" This function extracts position of the player. Can return multiple values if the player played in \n",
    "    different clubs or teams as different position. Hence the return type is list.\"\"\"\n",
    "\n",
    "\n",
    "    # This is look up of all postions available in football\n",
    "    pos = [\"forward\", \"captian\", \"attacking midfielder\", \"striker\", \"winger\", \"central midfielder\", \"defensive tackle\", \"defensive end\"]\n",
    "    player_position = []\n",
    "    try:\n",
    "        sent = sent_tokenize(doc)\n",
    "        for i, sent in enumerate(sent):\n",
    "            for x in pos:\n",
    "                # Find matching in look up and sentence.\n",
    "                regex = re.compile(r'\\b({0})\\b'.format(x), flags=re.IGNORECASE)\n",
    "                \n",
    "                # if result is true append to a list\n",
    "                r = bool(regex.search(sent))\n",
    "                \n",
    "                if r == True:\n",
    "                    player_position.append(x)\n",
    "        \n",
    "        return list(set(player_position))\n",
    "    except:\n",
    "        print(\"Please make sure that input to the function is string\")\n",
    "\n",
    "try:\n",
    "    print(position_of_the_player(list_of_doc[0]))\n",
    "except:\n",
    "    print(\"Maximum length of document is 10 i.e. index 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task 5 - Json-ld\n",
    "\n",
    "Here, first we need to generate the data, because arguments is given as list to generate_jsonld function. I have provided with two solution.\n",
    "    \n",
    "<b>Solution 1:</b> Simple store the required format in a variable as give the arguments where required. However, not sure wheteher the return type needed is linked data format. Hence given another solution using pyld to get Json ld.\n",
    "    \n",
    "<b>Solution 2:</b> Used Pyld to generated Json ld. It contains context and doc - to use this data as linked data then contex and doc is necessay. However, in given format in assignment dosent contain context section. Hence poping context from json ld (not sure if this is allowed - linked data needs context).\n",
    "\n",
    "\n",
    "\n",
    "Below is the data generaton function to convert all arguments to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cristiano Ronaldo dos Santos', '5 February 1985 ', 'Portugal', ['forward'], ['Real Madrid', 'Portugal national team']]\n"
     ]
    }
   ],
   "source": [
    "def data_generator(doc):\n",
    "    \n",
    "    data = [name_of_the_player(doc), date_of_birth(doc), country_of_origin(doc), position_of_the_player(doc), team_of_the_player(doc)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "data = data_generator(list_of_doc[0])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"@id\": \"http://my-soccer-ontology.com/footballer/Cristiano Ronaldo dos Santos\", \"name\": \"Cristiano Ronaldo dos Santos\", \"born\": \"5 February 1985 \", \"country\": \"Portugal\", \"position\": [{\"@id\": \"http://my-soccer-ontology.com/position/\", \"type\": [\"forward\"]}], \"team\": [{\"@id\": \"http://my-soccer-ontology.com/team/\", \"name\": [\"Real Madrid\", \"Portugal national team\"]}]}\n"
     ]
    }
   ],
   "source": [
    "def generate_jsonld1(arg, con=True):\n",
    "    if con == True:\n",
    "\n",
    "        ld = { \"@id\": \"http://my-soccer-ontology.com/footballer/\"+arg[0],\n",
    "\n",
    "            \"name\": arg[0],\n",
    "            \"born\": arg[1],\n",
    "            \"country\": arg[2],\n",
    "            \"position\": [\n",
    "                { \"@id\": \"http://my-soccer-ontology.com/position/\",\n",
    "                    \"type\": arg[3]\n",
    "                }\n",
    "             ],   \n",
    "             \"team\": [\n",
    "                { \"@id\": \"http://my-soccer-ontology.com/team/\",\n",
    "                    \"name\": arg[4]\n",
    "                }   \n",
    "             ]\n",
    "        }\n",
    "\n",
    "        return json.dumps(ld)\n",
    "    \n",
    "    elif con == False:\n",
    "        \n",
    "        ld = { \"@id\": \"http://my-soccer-ontology.com/footballer/\"+arg[0],\n",
    "\n",
    "            \"name\": arg[0],\n",
    "            \"born\": arg[1],\n",
    "            \"country\": arg[2],\n",
    "            \"position\": [\n",
    "                { \"@id\": \"http://my-soccer-ontology.com/position\",\n",
    "                    \"type\": arg[3]\n",
    "                }\n",
    "             ],   \n",
    "             \"team\": [\n",
    "                { \"@id\": \"http://my-soccer-ontology.com/team\",\n",
    "                    \"name\": arg[4]\n",
    "                },\n",
    "             ],\n",
    "            \"Debut Year\": arg[5][0],\n",
    "            \"Debut Age\": arg[5][1]\n",
    "        }\n",
    "\n",
    "        return json.dumps(ld)\n",
    "        \n",
    "\n",
    "\n",
    "try:\n",
    "    data = data_generator(list_of_doc[0])\n",
    "    print(generate_jsonld1(data))\n",
    "except:\n",
    "    print(\"Please make sure that list index given is right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'born': '5 February 1985 ', 'country': 'Portugal', ' @id': 'http://my-soccer-ontology.com/footballer/Cristiano Ronaldo dos Santos', 'name': 'Cristiano Ronaldo dos Santos', 'position': {'@id': 'http://my-soccer-ontology.com/position', 'type': 'forward'}, 'team': {'@id': 'http://my-soccer-ontology.com/team', 'name': ['Real Madrid', 'Portugal national team']}}\n"
     ]
    }
   ],
   "source": [
    "def generate_jsonld2(arg, con=True):\n",
    "    \n",
    "    if con == True:\n",
    "        doc = {\n",
    "\n",
    "            \"http://my-soccer-ontology.com/footballer/name_of_the_player\":  \n",
    "            \"http://my-soccer-ontology.com/footballer/\" +arg[0],\n",
    "            \"http://my-soccer-ontology.com/name\": arg[0],\n",
    "            \"http://my-soccer-ontology.com/born\": arg[1],\n",
    "            \"http://my-soccer-ontology.com/country\": arg[2],\n",
    "            \"http://my-soccer-ontology.com/team\": {\"@id\": \"http://my-soccer-ontology.com/team\", \"@type\": arg[4]},\n",
    "            \"http://my-soccer-ontology.com/position\": {\"@id\": \"http://my-soccer-ontology.com/position\", \"@type\": arg[3]}\n",
    "        }\n",
    "\n",
    "        context = {\n",
    "            \" @id\": \"http://my-soccer-ontology.com/footballer/name_of_the_player\", \"name\": \"http://my-soccer-ontology.com/name\",\n",
    "            \"born\": \"http://my-soccer-ontology.com/born\", \"country\": \"http://my-soccer-ontology.com/country\",\n",
    "            \"team\": {\"@id\": \"http://my-soccer-ontology.com/team\", \"@type\": \"@id\"}, \n",
    "            \"position\": {\"@id\": \"http://my-soccer-ontology.com/position\", \"@type\": \"@id\"}\n",
    "        }\n",
    "        \n",
    "        compacted = jsonld.compact(doc, context)\n",
    "        \n",
    "        # Data preprocess to look like the format given in assignment question.\n",
    "        # Pop the context and renaming several attributes:\n",
    "        compacted.pop('@context')\n",
    "        compacted['position']['type'] = compacted['position'].pop('@type')\n",
    "        compacted['team']['name'] = compacted['team'].pop('@type')\n",
    "        \n",
    "        # Cleaning the text containing '/' character\n",
    "        if type(compacted['position']['type']) == str:\n",
    "            compacted['position']['type'] = compacted['position']['type'].replace(\"/\", \"\")\n",
    "        else:\n",
    "            compacted['position']['type'] = [position.replace(\"/\", \"\") for position in compacted['position']['type']]\n",
    "        \n",
    "        if type(compacted['team']['name']) == str:\n",
    "            compacted['team']['name'] = compacted['team']['name'].replace(\"/\", \"\")\n",
    "        else:\n",
    "            compacted['team']['name'] = [name.replace(\"/\", \"\") for name in compacted['team']['name']]\n",
    "        \n",
    "        return compacted\n",
    "    \n",
    "    if con == False:\n",
    "            \n",
    "        doc = {\n",
    "\n",
    "            \"http://my-soccer-ontology.com/footballer/name_of_the_player\":  \n",
    "            \"http://my-soccer-ontology.com/footballer/\" +arg[0],\n",
    "            \"http://my-soccer-ontology.com/name\": arg[0],\n",
    "            \"http://my-soccer-ontology.com/born\": arg[1],\n",
    "            \"http://my-soccer-ontology.com/country\": arg[2],\n",
    "            \"http://my-soccer-ontology.com/debutYear\": arg[5][0],\n",
    "            \"http://my-soccer-ontology.com/debutAge\": arg[5][1],\n",
    "            \"http://my-soccer-ontology.com/team\": {\"@id\": \"http://my-soccer-ontology.com/team\", \"@type\": arg[4]},\n",
    "            \"http://my-soccer-ontology.com/position\": {\"@id\": \"http://my-soccer-ontology.com/position\", \"@type\": arg[3]}\n",
    "        }\n",
    "\n",
    "        context = {\n",
    "            \" @id\": \"http://my-soccer-ontology.com/footballer/name_of_the_player\", \"name\": \"http://my-soccer-ontology.com/name\",\n",
    "            \"born\": \"http://my-soccer-ontology.com/born\", \"country\": \"http://my-soccer-ontology.com/country\",\n",
    "            \"debutYear\": \"http://my-soccer-ontology.com/debutYear\", \"debutAge\":\"http://my-soccer-ontology.com/debutAge\",\n",
    "            \"team\": {\"@id\": \"http://my-soccer-ontology.com/team\", \"@type\": \"@id\"}, \n",
    "            \"position\": {\"@id\": \"http://my-soccer-ontology.com/position\", \"@type\": \"@id\"}\n",
    "        }\n",
    "        \n",
    "        compacted = jsonld.compact(doc, context)\n",
    "        \n",
    "        # Data preprocess to look like the format given in assignment question.\n",
    "        # Pop the context and renaming several attributes:\n",
    "        compacted.pop('@context')\n",
    "        compacted['position']['type'] = compacted['position'].pop('@type')\n",
    "        compacted['team']['name'] = compacted['team'].pop('@type')\n",
    "        \n",
    "        # Cleaning the text containing '/' character\n",
    "        if type(compacted['position']['type']) == str:\n",
    "            compacted['position']['type'] = compacted['position']['type'].replace(\"/\", \"\")\n",
    "        else:\n",
    "            compacted['position']['type'] = [position.replace(\"/\", \"\") for position in compacted['position']['type']]\n",
    "        \n",
    "        if type(compacted['team']['name']) == str:\n",
    "            compacted['team']['name'] = compacted['team']['name'].replace(\"/\", \"\")\n",
    "        else:\n",
    "            compacted['team']['name'] = [name.replace(\"/\", \"\") for name in compacted['team']['name']]\n",
    "        \n",
    "        return compacted\n",
    "\n",
    "    \n",
    "data = data_generator(list_of_doc[0])\n",
    "print(generate_jsonld2(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 - Debut Year and Debut Age\n",
    "\n",
    "I have take debut year and debut age of the player. However, it is important to note that, these two values are not available for all the players. Hence, if debut year or debut age is not present I have added as \"Not Available\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2004', '17']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relation_debutYearAge(doc):\n",
    "    \n",
    "    \"\"\"This function extracts the debut year and debut age of the player. \n",
    "    Note - this information if not available for all the players\"\"\"\n",
    "    \n",
    "    sent = sent_tokenize(doc)\n",
    "    deb = []\n",
    "    ag = []\n",
    "    # For each sentence check if the debut key is present or not\n",
    "    for se in sent:\n",
    "        sp_sent = se.split()\n",
    "        \n",
    "        if \"debut\" in sp_sent:\n",
    "            # If debut year is present jecy for 4 digit number.\n",
    "            date = re.findall('\\d{4}', \" \".join(sp_sent))\n",
    "            if len(date) !=0:\n",
    "                deb.append(date[0])\n",
    "        \n",
    "        # Check for debut and age key in a sentence - \n",
    "        if \"debut\" in sp_sent and \"aged\" in sp_sent:\n",
    "            # Search for 2 digit number and append to the list.\n",
    "            age = re.findall('\\d{2}', \" \".join(sp_sent))\n",
    "            ag.append(age[0])       \n",
    "    \n",
    "    # If the above process dosent return anything, insert year and age is not available.\n",
    "    if len(deb) == 0:\n",
    "        deb.append(\"Not Available\")\n",
    "    if len(ag) == 0:\n",
    "        ag.append(\"Not Available\")\n",
    "\n",
    "    return [deb[0], ag[0]]\n",
    "\n",
    "relation_debutYearAge(list_of_doc[1])\n",
    "\n",
    "\n",
    "# Note: Debut year or debut age is not available for all the sntences (3,7). Moreover, some of the sentence has only\n",
    "# debut age, while the others has only debut year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cristiano Ronaldo dos Santos', '5 February 1985 ', 'Portugal', ['forward'], ['Real Madrid', 'Portugal national team'], ['2003', 'Not Available']]\n"
     ]
    }
   ],
   "source": [
    "def data_generator(doc):\n",
    "    \n",
    "    data = [name_of_the_player(doc), date_of_birth(doc), country_of_origin(doc), position_of_the_player(doc), team_of_the_player(doc), relation_debutYearAge(doc)]\n",
    "    return data\n",
    "\n",
    "\n",
    "data = data_generator(list_of_doc[0])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json-Ld from solution1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"@id\": \"http://my-soccer-ontology.com/footballer/Cristiano Ronaldo dos Santos\", \"name\": \"Cristiano Ronaldo dos Santos\", \"born\": \"5 February 1985 \", \"country\": \"Portugal\", \"position\": [{\"@id\": \"http://my-soccer-ontology.com/position\", \"type\": [\"forward\"]}], \"team\": [{\"@id\": \"http://my-soccer-ontology.com/team\", \"name\": [\"Real Madrid\", \"Portugal national team\"]}], \"Debut Year\": \"2003\", \"Debut Age\": \"Not Available\"}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_jsonld1(data, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json-Ld from solution2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' @id': 'http://my-soccer-ontology.com/footballer/Cristiano Ronaldo dos Santos',\n",
       " 'born': '5 February 1985 ',\n",
       " 'country': 'Portugal',\n",
       " 'debutAge': 'Not Available',\n",
       " 'debutYear': '2003',\n",
       " 'name': 'Cristiano Ronaldo dos Santos',\n",
       " 'position': {'@id': 'http://my-soccer-ontology.com/position',\n",
       "  'type': 'forward'},\n",
       " 'team': {'@id': 'http://my-soccer-ontology.com/team',\n",
       "  'name': ['Real Madrid', 'Portugal national team']}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_jsonld2(data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
